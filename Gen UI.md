
Hello everybody, Adam Lusk here and today we're going to be talking about generative UI or using things like large language models to create personalized realtime user experiences on the web. This is an interesting topic to cover because if you think about how you and I interact with computers on a day-to-day basis, a majority of that time is likely spent in some sort of web browsing setting on the internet looking at websites and anything in between. And if you're not a front-end dev or a user experience researcher, you might not even understand the amount of effort and time that goes into actually crafting the different web experiences that we see today. Every website is made with the ideas of user experience and user interface in mind. User experience tends to cover things like interaction and navigation of websites, how information is actually displayed to the user or delivered to the user in some sort of manner. Whereas user interface design is usually around things like the visuals, how the website actually looks and the different colors and typography, things like that. But these all usually come together with some sort of purpose or goal in mind. If we think of the UI and UX of something like apple.com, which undoubtedly has had many thousands and thousands of hours of people's time put into actually crafting this website and how someone interacts with it. We know that the goal of Apple is to sell Apple products. So, of course, they have things like shop iPhone or learn more. Learn more might be for, you know, if I'm curious of buying an iPhone but don't know much about it, I might click that. But an easy way to get shopping. And even when I click on that, it brings me directly to this where we can start to see fun visuals. And as I scroll, things are popping up. and it's a very, you know, specifically crafted experience to showcase their product and try and get me to get a little closer to actually buying one. But one of the immediate issues here that comes to mind when thinking about UI and UX design is that you have to design for everybody that's coming into your website. And as a result, you need to service everybody with one common approach. This shows an immediate fallacy to me when going to something like apple.com because looking at my own life experience, I already have an iPhone. I'm not interested in learning more about iPhones or shopping an iPhone. And to Apple, they already have me as a customer. So, showing me something like an iPhone, especially a 16, which I have a 16, is pretty useless information to me. I also even have an Apple Watch, so I don't have a Series 10, so that might be a little bit more interesting to me to see, but you can start to see where this breaks down as you're designing for a broad audience rather than an individual personalized level. And so that's of course where something like generative UI actually starts to come into play. Using things like language models to dynamically choose and deliver web-based experiences based on what we might know about a user or in, you know, Apple's case, a customer visiting a website or doing some sort of online experience. In this blog post by Mark O'Neal, he has a very nice little mockup of a potential diagram of how something like generative UI would work where you take all of the initial context that you might have, things like instructions, which could be prompts, behavior, user data, maybe even current screen captures, pop it into some form of artificial intelligence systems that uses generative based models and using the generativebased models actually either populate, select, or display different components. to or screens to a user on the fly. Or going back to the original definition that I was showing earlier, a user interface that is dynamically generated in real time by AI to provide an experience customized to fit the user's needs and context. which in the case of Apple might mean that you know if I were to go to apple.com rather than showing me something like the iPhone it might be personalized to my needs, wants and ideas and maybe show me something like I don't know a Mac Mini that has a lot of RAM and neural cores. Now this is not to be confused with actually using Genai tools to create UX or UI designs. Well, it is understood and of course backed up by this Google research paper that using things like chat GPT or other language modelbased assistants can expedite the process of UI and UX design both ideulating about things like experience flows or maybe even writing some of the front-end code to actually display the interface to a user or do some prototyping in a sense that is pretty different than what we're trying to achieve here which is the real-time personalization of a web experience using a link language model. Now, there have been a few demonstrations of things like chatbased assistance with generative UI. If we look at this versel demo and I say something like what's the stock price of Apple? What it can do is actually create a component and populate it with this information which will show me the stock price of Apple here. And even looking at something like deep research from chat GBT and OpenAI, you might even think about this as generative UI as well because as it is performing this task and putting together and having this conversation with me, it's also generating some pieces of information to display at the same time here. But one of the things you'll see is a lot of these generative UI based demonstrations are very heavily focused on the chat experience. So, I wanted to put together something that still includes chat for the sake of actual demonstration purposes, but is a little bit more interface focused. So, the demonstration that we'll be going over today is one that I put together called more of the GenUI product assistant. Inspired by how people like Apple display and show their products and the kind of selection criteria of going through products, I wanted to show more of a focus on the actual interface here alongside still including some of these chat ideas. So, right here we have a demonstration that has a whole bunch of different laptops put together. And what we're going to do is go through with a language model and display different things about laptops and try and maybe select one that would be best for me. But before we show the live demo, I do want to give props. Much of this is a fork of this repo here from Lang Chain and Brace Sprout here who put together this Gen UI Python example here. I will be showing some of the backend, but we'll be glossing over a lot of the specifics. So, I certainly recommend and we'll be linking in the description below checking out Lang Chain's videos. They have a couple here, some with Python, some with JavaScript for building generative UI apps. And of course, much of the code that you're going to see today is repurposed from this repo here. So, a big shout out to Brace and the Langchain team for putting together these and furthering the ability of folks in the community to have nice, repeatable, and easy to use code. But for that, let's quickly show an example of what this is able to do and then dive into how it actually works. The first thing though before that is I do have a little bit of personalization loaded in here. So, this whole experience is going to be based around kind of what we know about the current user interacting with it. And so, in the back here, I have this persona loaded. And this persona is going to be Alex Thompson. A 28-year-old software developer prefers things like lightweight laptops, needs some high performance for coding and development, is a little budget conscious, but looking for, you know, value, and then also some things that you might expect to have captured in the back as part of your web analytics. things like browsing history, things like search history, maybe even if it's a returning customer like Alex might be buying a second laptop, you might have some ideas around their past purchase history. So, this is really just trying to simulate all of the information in a more naive, exaggerated way that you might be able to aggregate and collect in the back of a website to use towards actually personalizing an experience here. So, hopping back over to the interface, what I can do is just say hi. And you'll see immediately that we get a response streamed back to us that says, "Hey, Alex, how can I help you with your next laptop purchase today? You're looking for something lightweight or something with Linux compatibility?" And of course, this is all going to be personalized immediately off of my persona here. So, you can already see that we're able to do things like incorporate names and really start to provide that personalized experience. Whereas a general chatbot that doesn't know anything about me might not have something like this available. And so I would say, you know, looking for some mid-range laptops. And you'll notice that something's happened right when I said that. The right hand side here has updated dramatically and we've gotten a little bit of a larger response here on the left. Now, what's particularly compelling about this is, of course, the system has kept in mind my preferences and what I'm looking for and is displaying the actual experience as such. So, over here on the right, I have mid-range laptops for developers. It knows that I am a software engineer in this persona, and it's going to even put in some copy around having things like lightweight designs and Linux compatibility, which in my persona is something that I'm looking for. And out of the whole entire selection of laptops that we have, it's chosen these to display to me. All of the chat here in following in line with this is going to be, of course, keeping in mind all of my preferences. But some of the interesting things that we can start to do here now is guide the user, which in this case is Mr. Alex here, down the path of actually selecting one of these laptops. And you'll see that the LLM has actually responded with something like, you know, do you have a preferred screen size or do you want to focus on models with things like 32 GB of RAM for even more futurep proofing? So I might say actually, you know, 32GB RAM is a good idea. And because I said that, what it's going to do is actually recommend one specific laptop here. And of course, the chat keeps up as this conversation evolves. It's going to have some different considerations. things like the kilograms is heavier than my previous MacBook and my XPS laptop here as well as the price being on the higher end but with some pitches like we're getting into workstation level specs and so it's also recommending some next steps. So does this level of power and screen size fit my workflow? Do I prefer something lighter or more portable? Would I like to see more options with 32 GB of RAM? Now the Razer Blade 16 is a very pricey laptop. So I might say, you know, what's something similar but not as expensive and see what it says. So it's going to give me a couple of recommendations here, but then it's also going to provide again this different view, keeping in mind still that these laptops are affordable alternatives with 32 GB of RAM potential. all stuff that we've chatted about in this chat over here. And it's going to continue to try and match my preferences alongside here. And so I might say something like, you know, what's the difference between the ThinkPad X1 and the Razer Blade? And in saying this, it might give me something of a product comparison. So, I can go through and see all of the the different specs and how they might line up across each other and also continue to read some of this personalized copy. But all of this experience up until this point, as you can see, is very focused around dealing with and serving the user, which in our case is, of course, this Alex persona that we've put together, and showing everything that we can in line with what might best service her. So to end off this quick demonstration, I might say something like, "Actually, I think I like the ThinkPad." And it might do something like just display this single component here again with the ThinkPad specs and the ability to view the product. Now the cool part about everything that you've seen is every single time the screen on the right has been updated not the chat here that has all been completely large language model le. So there are specific components which you've seen the tiles, the individual product and then the product comparison that are available to the language model to provide. But the actual choice of what products to put in there and what to actually have the different copy and explanations here for is fully up to the language model based on the different personalization things like my persona as well as how the chat is progressing here. that fully guides the experience and interface that we're seeing on this part to the right. And then looking at something like this, the thoughts and ideas behind this are a lot more compelling to me than something like a standardized web experience like what we went over on something like apple.com where it doesn't show doesn't take into account my preferences, maybe anything else that I've done in the past or browsed. It's just showing me these very generic things. Whereas something like this can be the complete opposite of that. Offering that white glove level of service to someone like Mr. Alex here who is going over all of these different laptops and is interested in buying one as well as taking into account all of the different data points that we might have about Alex to hyperpersonalize this user experience. And there's a lot of cool things you can do with this. Something like the startup thesis here actually put together a example of even more generative UI where you can see that it is composing a response to show the celestial objects of space by putting together these custom things where I can click on them. It has different follow-up queries, different tabs, all sorts of these. But looking back at this original diagram that I keep referencing, you can imagine a future where maybe even the chat interface isn't something that we exactly need. A future with Gen UI might mean that I might just go to something like apple.com and it'll already have or create this personalized experience for me on the fly as my needs, wants, and interactions evolve. and not having chat I feel is a little bit more compelling because if you look at something like this which is a paper against generative UI which many of these points I don't actually particularly agree with they do however say something that does ring a bell that a GenUI system would rely on the user's ability to actually articulate their needs which is sometimes challenging in my demo here I explicitly say I'm looking for you know mid-range laptops and 32 GB of RAM which in my case I know what all of these things are but Let's say someone like maybe you know your parents who might not be so techsavvy are looking for a laptop. They might not have any clue about what RAM is and what they're actually maybe even looking for. Which is where these ideas of having things like related queries where you can actually follow up and it's generated also by the system not necessarily explicitly through chat and also using something like a language model which can aggregate all of this fuzzy data and also non-specific data to try and do these recommendations can still address that but maybe not at a large scale level. And the other criticism here too that I think is worth pointing out is that this does also require users to share vast amounts of their personal data with the system which as we know is already kind of being collected by a lot of these companies and by things like web browsers. But to get to that very specific personalized experience, especially with something like my product assistant here, it had to know things like my name, age, browsing, purchase history, and all these different personal data items that we might not want to give up to a website readily. However, the ideas are still there. And I do think that having something like a personalized web experience that's actually explicitly tailored to exactly what you want might make your day-to-day web browsing life a lot more efficient and a lot more effective. But with a lot of the theoretical and the benefits as well as the disadvantages out of the way, let's talk a little bit about what's going on behind the scenes and show some of the underlying systems. Of course, as I mentioned, do check out Lang Chain's videos here with Brace and his examples as a lot of the code is repurposed from there, and he goes through and gives a much more indepth overview of everything going on. To start, everything here is made using some combination of Versel's AI SDK, Nex.js, and Langchain. So you can see within my IDE I have a lang serve server as the backend that's just being hosted through something like fast API and then the front end is a nextjs server. So we actually have both a python backend and then a typescript javascript front end going on here. This could be simplified all into JavaScript using something like Langchain's JavaScript packages. But I am more familiar with Python. So I chose this kind of decoupled backend server frontend server communication architecture. So to start the main components here that we're going to want to look at is the graph of course the langraph. And langraph if you're unfamiliar is a way of chaining together language model calls in very deterministic ways. Creating an application with Langraph allows you to almost define exactly how information flows from one step to another. What actions are taken and how that data is transformed all within a state graph type of situation. This has become one of the de facto standards of creating more reliable agentic systems with language models. And I have plenty of other videos that use langraph in the back. So I'm going to assume some familiarity with how langraph actually works. Now looking at how our graph is getting compiled at the end here, it's not going to be anything super fancy. We're going to have a simple entrance of actually just invoking a language model and then based on that language model invocation, it's either going to be sent straight to the user or it might choose to call some sort of tools using regular function calling. And when a tool is used, there will also be a follow-up message that provides the context from the tools back to the language model to create a final response. Much of everything that's going on in this chain.py file is just going to be doing things like loading product catalogs, loading chat histories, and appending chat history so that we can keep everything nicely organized. I'm not using any fancy database setup which I would usually do for production level use cases here for things like chat and chat history. It's just going to be stored in a CSV file. As for things like the actual products, these are just going to be in folders like this. So I have this laptops folder that has a bunch of these images of the laptops as well as different. So let's say for four it's this one. And then four, I have just a copy of their front page for this specific laptop. And then my catalog here is a CSV file that includes things like product IDs, names, brand, CPU, family, all these good specs so that we can just display this nicely and easily. Not using anything fancy like a database or anything, just this CSV file here. Then back in our chain here, we can see that our state is going to take inputs as human messages. This is of course going to be a typing that comes directly from lang chain that's going to handle actually putting the message just the raw text into a format that can be passed through to something like the open AI API a placeholder for tool calls tool results as well as final responses and then we get to our first node which is going to invoke the model it's going to take in the state and it is going to essentially just create the prompt pass together the system prompt as well as different things like the product types which can be changed out if wanted pass it off to open AAI's 4.1 model along with a selection of tools to get the results here. The tool calling is actually what's going to be driving a lot of the dynamic interfaces that we see pop up during the experience. So I'll spend a little bit more time there. But to close out, the rest of the nodes that we have is either invoking tools or returning, mapping those tools to the actual functions that call them and generating final responses with all of the history of our tool calls to get a final response. And then of course this is all put together into a nice graph. But the more interesting part here, this is more of a standard, you know, back and forth chat flow with tool calling are the tools in and of themselves. Now, within my system prompt, which I hold in a config file that kind of gets routed to wherever it needs to go, I have three tools described here. A product details tool, a product comparison tool, and a product tiles tool. Now, we saw the tiles comparison and details tool within our demonstration. But just to go over the product details one is showing exactly one product similar to how it showed the Razer laptop. comparison is of course going to be the two together with the different specs and the tiles are going to be when it displays three or more products in that tiled list. And the rest of this prompt is just going to be guiding towards clinging clarifying questions and guiding the user towards selecting a specific laptop as well as some more specifics around when to be using these different tools. If we look at something like the product comparison tool, that is going to be held here. And of course, if you're familiar with defining tools in Langchain, these can just be arbitrary Python functions. And we have this set up as such. So it takes in a product ID, a second product ID as well as a generated description. And we can see from the input schema that we have here for the arguments that we are describing to for the language model to insert the product ID 1, product ID 2 as well as generative content to display with the product type comparison. These IDs are just coming from the CSV where we have a product ID that's simply just a 1 to 10 list of the 10 available laptops in this specific example. The rest of this tool will verify that everything is entered in correctly, but most importantly return back in JSON format the different products as well as the images, the image URLs, the different marketing content and descriptions and where to find all of this. And so this JSON object is going to be sent out and returned as comparison data. And the rest of the tools tend to follow something like this where it just takes in product IDs and descriptions. We'll verify that and then return back the necessary information in this dictionary format. So this is all going to be packaged up as a simple lang serve server here using just fast API and unicorn. Nothing out of the ordinary for a backend. We can verify by going to the localhost 8000 docs here to get the fast API documentation. Then we have things like our chat and our chat config as well as some of the other things that I have like the user profile API which will load and post the user profile that we have in just raw text. Now all of these individual tools also have corresponding components here that are written in Typescript. This is where we're going to do two things. We're going to have a loading UI that's just going to be a loading one where when a tool is called, it'll actually start to display this before it's fully populated. Although this works pretty fast, so the loading wasn't necessarily needed for our specific demonstration. And then a full comparison component that returns back all of the good stuff to actually make sure that what we want which is that comparison experience be displayed on the screen. And this will of course be populated by the information that's provided from the tool call as that tool call is executed. In our main agent file of the front end, we'll see that we are defining things like the tool components and mapping the different tool component names to their both loading and final components. And so what this will allow us to do is as the results from the language model are streamed out into our front-end server, we can see if there are any tool calls, map those tool calls to the selected components, and then create a streamable UI, which will allow us to actually stream updates to our interface, which we first put the loading component into. And then what we do is we take all of the arguments to invoke the tool, populate that and then update it with the final component back to the user. This is also where we will be handling the models output response streaming as well. So at a high level when we actually send a message in our interface there, it gets sent to the langraph chain where that chain will either create a response or call a tool. When a tool is called, our front end will recognize that a tool is being called. Map that tool called name to the loading component. Display that loading component. Wait for the actual arguments from our language models tool called results to be sent out. And then put in the final component with all of the updated information and the correct information. Finally there. At the same time, we're still handling the language models general text and putting it into the textbox response as streamed text. And while all of this is going on, we still have the personalization data and everything loaded in the background as well as the custom system prompting to make sure that this is all a specific experience that encourages actually changing and updating the front end to display the most relevant information visually during this experience. So, with that, that just about covers a lot of the big ideas as well as the demonstration of this specific example here that I wanted to cover today. I'll have all of the code as well as all of the credits towards people like Brace and Langchain down in the description below. But if you're interested in trying this out, you can go ahead and check out my repo here where I have a little bit more information as well as everything available for you to download. check out, fork, run, all of that good stuff. But with that, I hope I got you thinking about the future of web experiences and how using things like language models to enable real time hyperpersonalization can be super powerful and super interesting, especially when combined with different things like chat or completely taking chat out of the question and just using personalization data to create specific web experiences. Now, the next time you go on a website, think a little bit about how things are laid out and how you're actually navigating that website and what might be better for you and how you might be able to apply things like language models or different AI models to better predict and provide that personalized experience. With that being said, hope you learned something interesting today. If you like the video, like the video. If you have any questions, leave them down in the comments below. If you want to see more, consider subscribing. And if you want to support the channel even further, consider leaving a super thanks donation or joining a membership.
